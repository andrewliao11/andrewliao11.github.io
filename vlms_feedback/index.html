<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <title>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</title> <meta name="description" content="Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?"> <meta name="keywords" content="vision-language models, visual grounding, prompt engineering, feedback"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta property="og:title" content="Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?"> <meta property="og:type" content="website"> <meta property="og:site_name" content="Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?"> <meta property="og:image" content=""> <meta property="og:image:type" content="image/png"> <meta property="og:image:width" content="1082"> <meta property="og:image:height" content="639"> <meta property="og:url" content=""> <meta property="og:description" content="Project page for Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?"> <meta name="twitter:title" content="Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?"> <meta name="twitter:description" content="Project page for Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?"> <meta name="twitter:image" content=""> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="/assets/external_pages/label_transfer/static/css/bulma.min.css"> <link rel="stylesheet" href="/assets/external_pages/label_transfer/static/css/bulma-carousel.min.css"> <link rel="stylesheet" href="/assets/external_pages/label_transfer/static/css/bulma-slider.min.css"> <link rel="stylesheet" href="/assets/external_pages/label_transfer/static/css/fontawesome.all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel="stylesheet" href="/assets/external_pages/label_transfer/static/css/index.css"> <link rel="icon" href="/assets/img/logo.jpg"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script defer src="/assets/external_pages/label_transfer/static/js/fontawesome.all.min.js"></script> <script src="/assets/external_pages/label_transfer/static/js/bulma-carousel.min.js"></script> <script src="/assets/external_pages/label_transfer/static/js/bulma-slider.min.js"></script> <script src="/assets/external_pages/label_transfer/static/js/index.js"></script> </head> <body> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title">Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://andrewliao11.github.io">Yuan-Hong Liao</a><sup>1</sup>,</span> <span class="author-block"> <a href="https://rafidrm.github.io" rel="external nofollow noopener" target="_blank">Rafid Mahmood</a><sup>2, 3</sup>,</span> <span class="author-block"> <a href="https://www.cs.utoronto.ca/~fidler/" rel="external nofollow noopener" target="_blank">Sanja Fidler</a><sup>1, 2</sup>,</span> <span class="author-block"> <a href="http://www.cs.toronto.edu/~davidj/" rel="external nofollow noopener" target="_blank">David Acuna</a><sup>2</sup></span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"><sup>1</sup>University of Toronto, Vector Institute, </span> <span class="author-block"><sup>2</sup>NVIDIA,</span> <span class="author-block"><sup>3</sup>University of Ottawa</span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span> <span class="link-block"> <a href="" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="hero teaser"> <div class="container is-max-desktop"> <div class="hero-body"> <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%"> <source src="/assets/video/vlms_feedback/website_teaser_gif_vlm_feedback.mp4" type="video/mp4"></source> </video> <h2 class="subtitle has-text-centered"> Can VLMs improve their responses by taking <i>self-generated</i> feedback? </h2> </div> </div> </section> <section class="section" id="abstract"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2 class="title is-3">Abstract</h2> <div class="content has-text-justified"> <p> Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modi- fying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by "listening" to feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can listen to feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. </p> </div> </div> </div> </div> </section> <section class="hero is-light is-small" id="semi_interactive_demo"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column"> <h2 class="title is-3">Give it a try! Provide your feedback to our VLM and see if it's doing well.</h2> <div class="columns is-centered"> <div class="column content"> <p>Here, you can interact with <a href="https://huggingface.co/liuhaotian/llava-v1.5-13b" rel="external nofollow noopener" target="_blank">LLaVA-1.5 13b model</a> by providing binary feedback.</p> <img src="/assets/img/vlms_feedback/demo_placeholder.png"> <p><small><sup>*</sup>All results are pre-generated.</small></p> </div> </div> </div> </div> </div> </div> </section> <section class="section" id="feedback_dynamics"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h2 class="title is-3">Feedback Dynamics in VLMs</h2> <p>In this paper, we perform the first exploration of whether VLMs can improve their semantic grounding on zero-shot tasks by only "listening" to automated prompt-based feedback. This interaction requires no additional fine-tuning and can be achieved by API-based VLMs for the downstream applications. We summarize the contribution as follow:</p> <div style="padding-left: 24px"> <ol> <li> <b>VLMs can listen to feedback to improve downstream visual grounding</b>: In a single-step setting with a noise-free binary signal, VLMs improve their semantic grounding performance by 4 to 12 accuracy points, and over multiple rounds, by over 15 points across five rounds. This shows the potential of feedback as a means of improving grounding performance in VLMs.</li> <li> <b>VLMs can be used as binary feedback providers</b>: Similar to the findings shown in the prior work (Huang et al., 2023), our experimental results show that VLMs cannot self-correct (yet). However, we find that this does not imply VLMs cannot self-evaluate (Kadavath et al., 2022). We show that VLMs can provide high-quality binary feedback through isolation or marking of objects.</li> <li> <b>VLMs benefit from automatic iterative feedback by improving semantic grounding accuracy up to nearly 5 accuracy points</b>: We formulate an iterative framework that improve semantic grounding in VLMs by combining VLM verification and VLM feedback listening. </li> </ol> </div> </div> </div> </div> </section> <section class="section" id="experiment"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h2 class="title is-3">Iterative Feedback Improves Semantic Grounding in VLMs</h2> <div class="content"> We introduce an iterative loop of dialogue between a VLM agent and Verifier. At the first time step, the VLM agent obtain base predictions for every scene. We then prompt Verifiery (the same VLM) to generate a binary feedback for every prediction. In the next time step, we provide this binary feedback to the VLM agent and ask it to re-generate predictions. We repeat these steps until Verifier agrees with the predictions. </div> <div class="container is-centered"> <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%"><source src="/assets/video/vlms_feedback/website_framework_flow_gif.mp4" type="video/mp4"></source> </video> </div> <h3 class="title is-4">Quantitative Results</h3> <div class="content"> We compare our approach, VLM binary verification, with intrinsic self-correction (Kim et al., 2023). We also compare with a noise-free version of our iterative approach, noise-free verification. The following table show the results on ADE20k<sup>*</sup><br><small><sup>*</sup>We use a subset of ADE20k validation images for evaluation.</small> <img src="/assets/img/vlms_feedback/ade_quantative_res.png" margin-left="auto" margin-right="auto"> <p> We highlight the performance difference <i>w.r.t.</i> the performance of the base predictions and if the performances are below the performance of the base predictions, we use <font color="#ff0000">red-colored font.</font> </p> </div> <h3 class="title is-4">Qualitative Results</h3> <div class="content"> <img src="/assets/img/vlms_feedback/additional_qualitative_res_gpt4v_1.jpg" margin-left="auto" margin-right="auto"> <p> Here, we show an example using GPT-4V &amp; SoM (Jianwei Yang et al., 2023) as the VLM. GPT-4V is able to identify what objects are in the image, but struggles to identify the mapping between numeric object IDs with objects. With self-generated feedback (from center to left), GPT-4V successfully corrects its own predictions. For more qualitative results, please refere to our paper. </p> </div> </div> </div> </div> </section> <section class="section" id="BibTeX"> <div class="container is-max-desktop content"> <h2 class="title">BibTeX</h2> <pre><code>to fill</code></pre> </div> </section> <section> <div class="container is-max-desktop content"> <footer class="footer"> <div class="content"> <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io" rel="external nofollow noopener" target="_blank">NeRFies</a> under a <a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="external nofollow noopener" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International</a> </p> </div> </footer> </div> </section> </body> </html>